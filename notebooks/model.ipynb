{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417b609e-33fd-4681-acaf-2f82856f543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/miniconda3/envs/gliner/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d7ab4c-3c51-43c7-9352-7949bfe9c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/model_hub/mdeberta-v3-base/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f91aa45-cf7c-4c7d-ac55-ff18199aa4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████████████████████████████████████████████████████████████████████████████| 198/198 [00:00<00:00, 2530.16it/s, Materializing param=encoder.rel_embeddings.weight]\n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: /data/model_hub/mdeberta-v3-base/\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "mask_predictions.classifier.bias           | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias      | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight        | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias                | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias                | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias          | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias            | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight          | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight         | UNEXPECTED |  | \n",
      "deberta.embeddings.word_embeddings._weight | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight    | UNEXPECTED |  | \n",
      "mask_predictions.dense.weight              | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModel.from_pretrained(\"/data/model_hub/mdeberta-v3-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10726f20-258c-4223-a858-088c979f1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "def make_mlp(input_dim: int, hidden_dim: int, output_dim: int, dropout: float = 0.1) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(hidden_dim, output_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpanBatch:\n",
    "    # Text\n",
    "    text_input_ids: torch.LongTensor         # (B, N)\n",
    "    text_attention_mask: torch.LongTensor    # (B, N)\n",
    "\n",
    "    # Labels (label+desc) as a batch-of-batches flattened to (B*M, L)\n",
    "    label_input_ids: torch.LongTensor        # (B*M, L)\n",
    "    label_attention_mask: torch.LongTensor   # (B*M, L)\n",
    "\n",
    "    # Number of labels per sample (M). Assumed fixed M across batch for simplicity.\n",
    "    num_labels: int\n",
    "\n",
    "    # Optional supervision (multi-label over spans):\n",
    "    # span_targets: (B, num_spans, M) with 0/1, or float in [0,1]\n",
    "    span_targets: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"Label->Text cross-attention. Q attends to H.\"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, h: torch.Tensor, h_key_padding_mask: Optional[torch.BoolTensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        q: (B, M, d)\n",
    "        h: (B, N, d)\n",
    "        h_key_padding_mask: (B, N) True for PAD (to mask out)\n",
    "        \"\"\"\n",
    "        ctx, _ = self.attn(query=q, key=h, value=h, key_padding_mask=h_key_padding_mask, need_weights=False)\n",
    "        return self.ln(q + ctx)\n",
    "\n",
    "\n",
    "class SpanEnumerator:\n",
    "    \"\"\"Enumerate all spans up to max_width for each sequence length N (excluding pads by attention mask later).\"\"\"\n",
    "    def __init__(self, max_width: int):\n",
    "        self.max_width = int(max_width)\n",
    "\n",
    "    def enumerate(self, seq_len: int, device: torch.device) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          start_idx: (S,)\n",
    "          end_idx:   (S,)\n",
    "          width:     (S,)  (end-start)\n",
    "        where S = sum_{w=1..max_width} (seq_len - w + 1)\n",
    "        \"\"\"\n",
    "        starts = []\n",
    "        ends = []\n",
    "        widths = []\n",
    "        for w in range(1, self.max_width + 1):\n",
    "            s = torch.arange(0, seq_len - w + 1, device=device, dtype=torch.long)\n",
    "            e = s + (w - 1)\n",
    "            starts.append(s)\n",
    "            ends.append(e)\n",
    "            widths.append(torch.full_like(s, w - 1))\n",
    "        start_idx = torch.cat(starts, dim=0)\n",
    "        end_idx = torch.cat(ends, dim=0)\n",
    "        width = torch.cat(widths, dim=0)\n",
    "        return start_idx, end_idx, width\n",
    "\n",
    "\n",
    "class DebertaSchemaSpanModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Main model:\n",
    "      Text encoder -> token reps\n",
    "      Label encoder -> label reps\n",
    "      Cross-attn fusion -> fused label reps\n",
    "      Span scoring -> logits(span, label)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = \"microsoft/mdeberta-v3-base\",\n",
    "        share_encoders: bool = True,\n",
    "        use_width_embedding: bool = True,\n",
    "        max_span_width: int = 12,\n",
    "        num_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.text_encoder = AutoModel.from_pretrained(backbone_name)\n",
    "        if share_encoders:\n",
    "            self.label_encoder = self.text_encoder\n",
    "        else:\n",
    "            self.label_encoder = AutoModel.from_pretrained(backbone_name)\n",
    "\n",
    "        d_model = self.text_encoder.config.hidden_size\n",
    "        self.d_model = d_model\n",
    "        self.max_span_width = int(max_span_width)\n",
    "        self.span_enum = SpanEnumerator(max_width=max_span_width)\n",
    "\n",
    "        self.fuse = CrossAttentionFusion(d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        self.use_width_embedding = bool(use_width_embedding)\n",
    "        if self.use_width_embedding:\n",
    "            self.width_emb = nn.Embedding(max_span_width, d_model)  # width in [0..max_span_width-1]\n",
    "            span_in = d_model * 2 + d_model\n",
    "        else:\n",
    "            span_in = d_model * 2\n",
    "\n",
    "        self.span_ffn = make_mlp(span_in, hidden_dim=d_model * 4, output_dim=d_model, dropout=dropout)\n",
    "\n",
    "        # Optional: bias term per label (helps calibration when many negatives)\n",
    "        self.label_bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _pool_cls(last_hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Use first token as CLS. Shape: (B, L, d) -> (B, d).\"\"\"\n",
    "        return last_hidden_state[:, 0, :]\n",
    "\n",
    "    def encode_text(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor) -> torch.Tensor:\n",
    "        out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return out.last_hidden_state  # (B, N, d)\n",
    "\n",
    "    def encode_labels(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.LongTensor,\n",
    "        batch_size: int,\n",
    "        num_labels: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids/attention_mask: (B*M, L)\n",
    "        returns Q: (B, M, d)\n",
    "        \"\"\"\n",
    "        out = self.label_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self._pool_cls(out.last_hidden_state)  # (B*M, d)\n",
    "        q = pooled.view(batch_size, num_labels, -1)     # (B, M, d)\n",
    "        return q\n",
    "\n",
    "    def forward(self, batch: SpanBatch) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          logits: (B, S, M) where S = num_spans based on N and max_span_width\n",
    "          loss: optional\n",
    "          span_mask: (B, S) valid spans under attention_mask\n",
    "          start_idx/end_idx/width: (S,)\n",
    "        \"\"\"\n",
    "        B, N = batch.text_input_ids.shape\n",
    "        M = int(batch.num_labels)\n",
    "        device = batch.text_input_ids.device\n",
    "\n",
    "        # 1) Encode text\n",
    "        H = self.encode_text(batch.text_input_ids, batch.text_attention_mask)  # (B, N, d)\n",
    "\n",
    "        # 2) Encode labels (label+desc)\n",
    "        Q = self.encode_labels(batch.label_input_ids, batch.label_attention_mask, batch_size=B, num_labels=M)  # (B, M, d)\n",
    "\n",
    "        # 3) Cross-attn fusion (labels attend to text)\n",
    "        # key_padding_mask expects True for padding positions\n",
    "        pad_mask = batch.text_attention_mask == 0  # (B, N) bool\n",
    "        Qf = self.fuse(Q, H, h_key_padding_mask=pad_mask)  # (B, M, d)\n",
    "\n",
    "        # 4) Enumerate spans (based on full N, later masked by attention_mask)\n",
    "        start_idx, end_idx, width = self.span_enum.enumerate(seq_len=N, device=device)  # (S,)\n",
    "        S = start_idx.numel()\n",
    "\n",
    "        # 5) Build span representations from token reps\n",
    "        # Gather start/end token reps: (B, S, d)\n",
    "        H_start = H.index_select(dim=1, index=start_idx)  # (B, S, d)\n",
    "        H_end = H.index_select(dim=1, index=end_idx)      # (B, S, d)\n",
    "\n",
    "        if self.use_width_embedding:\n",
    "            W = self.width_emb(width).unsqueeze(0).expand(B, S, self.d_model)  # (B, S, d)\n",
    "            span_in = torch.cat([H_start, H_end, W], dim=-1)                   # (B, S, 3d)\n",
    "        else:\n",
    "            span_in = torch.cat([H_start, H_end], dim=-1)                      # (B, S, 2d)\n",
    "\n",
    "        span_vec = self.span_ffn(span_in)  # (B, S, d)\n",
    "\n",
    "        # 6) Compute logits via dot-product with fused label vectors\n",
    "        # logits[b, s, m] = <span_vec[b, s], Qf[b, m]>\n",
    "        logits = torch.einsum(\"bsd,bmd->bsm\", span_vec, Qf) + self.label_bias  # (B, S, M)\n",
    "\n",
    "        # 7) Span validity mask (exclude spans that touch padding tokens)\n",
    "        # Valid if both start and end positions are within attention_mask=1\n",
    "        attn = batch.text_attention_mask.bool()  # (B, N)\n",
    "        valid_start = attn.index_select(dim=1, index=start_idx)  # (B, S)\n",
    "        valid_end = attn.index_select(dim=1, index=end_idx)      # (B, S)\n",
    "        span_mask = valid_start & valid_end                       # (B, S)\n",
    "\n",
    "        out: Dict[str, torch.Tensor] = {\n",
    "            \"logits\": logits,             # (B, S, M)\n",
    "            \"span_mask\": span_mask,       # (B, S)\n",
    "            \"start_idx\": start_idx,       # (S,)\n",
    "            \"end_idx\": end_idx,           # (S,)\n",
    "            \"width\": width,               # (S,)\n",
    "        }\n",
    "\n",
    "        # 8) Optional loss (multi-label BCE over spans x labels)\n",
    "        if batch.span_targets is not None:\n",
    "            # span_targets expected shape: (B, S, M)\n",
    "            if batch.span_targets.shape != logits.shape:\n",
    "                raise ValueError(f\"span_targets shape {batch.span_targets.shape} must match logits {logits.shape}\")\n",
    "\n",
    "            # Mask out invalid spans by setting them to ignore (we'll zero their loss weight)\n",
    "            # BCEWithLogitsLoss supports per-element weights.\n",
    "            weight = span_mask.unsqueeze(-1).float()  # (B, S, 1)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                logits,\n",
    "                batch.span_targets,\n",
    "                weight=weight,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            denom = weight.sum().clamp_min(1.0) * M\n",
    "            out[\"loss\"] = loss / denom\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bc098d-6ea0-435b-9758-39d7762d844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████████████████████████████████████████████████████████████████████████████| 198/198 [00:00<00:00, 2605.51it/s, Materializing param=encoder.rel_embeddings.weight]\n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: /data/model_hub/mdeberta-v3-base\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "mask_predictions.LayerNorm.bias            | UNEXPECTED |  | \n",
      "deberta.embeddings.word_embeddings._weight | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight          | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias                | UNEXPECTED |  | \n",
      "mask_predictions.dense.weight              | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias          | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight        | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias           | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias                | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias      | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight    | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight         | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "B, N = 2, 128\n",
    "M, L = 20, 64\n",
    "model = DebertaSchemaSpanModel(\n",
    "    backbone_name=\"/data/model_hub/mdeberta-v3-base\",\n",
    "    share_encoders=True,\n",
    "    use_width_embedding=True,\n",
    "    max_span_width=12,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1622b711-b30a-4608-afa5-ada435a4d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1470, 20])\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device  = \"cpu\"\n",
    "model.to(device).half()\n",
    "\n",
    "batch = SpanBatch(\n",
    "    text_input_ids=torch.randint(0, 1000, (B, N), device=device),\n",
    "    text_attention_mask=torch.ones(B, N, device=device, dtype=torch.long),\n",
    "    label_input_ids=torch.randint(0, 1000, (B * M, L), device=device),\n",
    "    label_attention_mask=torch.ones(B * M, L, device=device, dtype=torch.long),\n",
    "    num_labels=M,\n",
    "    span_targets=None,\n",
    ")\n",
    "\n",
    "out = model(batch)\n",
    "print(out[\"logits\"].shape)  # (B, S, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951414a3-8f62-4c91-aefc-146ff7c4ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(samples, tokenizer, max_span_len=12, neg_ratio=1, hard_bank=None):\n",
    "    # hard_bank: dict[(text_id,label)] -> list[(span, score)]  可选\n",
    "\n",
    "    texts = [s[\"sentence\"] if \"sentence\" in s else s[\"text\"] for s in samples]\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    label_instances = []\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        text = texts[i]\n",
    "        entities = s[\"entities\"]\n",
    "        label_desc_map = {d[\"label\"]: d[\"definitions\"] for d in s[\"description\"]}  # 你的格式\n",
    "\n",
    "        gold_by_label = group_by_label([\n",
    "            {\"start\": e[\"pos\"][0], \"end\": e[\"pos\"][1], \"label\": e[\"type\"]}\n",
    "            for e in entities\n",
    "        ])\n",
    "\n",
    "        # 1) 生成候选 spans（你可替换为更强的 proposal）\n",
    "        cand_spans = propose_spans(enc[\"offset_mapping\"][i], max_span_len=max_span_len)\n",
    "        # cand_spans: list[(char_s,char_e)]\n",
    "\n",
    "        # 2) 对该样本每个 label，生成一个 label_instance\n",
    "        for label, pos_spans in gold_by_label.items():\n",
    "            dx, desc = sample_desc(label_desc_map, label)\n",
    "\n",
    "            # 2.1 负例池：候选中排除与任一正例重叠\n",
    "            pos_set = pos_spans\n",
    "            neg_pool = [sp for sp in cand_spans if all(not overlap(sp, p) for p in pos_set)]\n",
    "\n",
    "            # 2.2 先随机负例\n",
    "            need_neg = max(len(pos_spans) * neg_ratio, 1)\n",
    "            neg_rand = random.sample(neg_pool, k=min(need_neg, len(neg_pool)))\n",
    "\n",
    "            # 2.3 hard negatives（如果有）\n",
    "            neg_hard = []\n",
    "            if hard_bank is not None:\n",
    "                key = (s.get(\"id\", i), label)\n",
    "                # hard_bank[key] = list[((s,e),score), ...]\n",
    "                if key in hard_bank:\n",
    "                    hard_sorted = [sp for (sp,score) in sorted(hard_bank[key], key=lambda x:-x[1])]\n",
    "                    # 排除与正例重叠\n",
    "                    hard_sorted = [sp for sp in hard_sorted if all(not overlap(sp,p) for p in pos_set)]\n",
    "                    neg_hard = hard_sorted[: min(need_neg, len(hard_sorted))]\n",
    "\n",
    "            # 2.4 混合：hard优先 + 少量随机补齐\n",
    "            neg_final = []\n",
    "            if neg_hard:\n",
    "                hard_take = int(need_neg * 0.7)\n",
    "                neg_final.extend(neg_hard[:hard_take])\n",
    "                rest = need_neg - len(neg_final)\n",
    "                if rest > 0:\n",
    "                    # 从随机里补齐\n",
    "                    neg_final.extend(neg_rand[:rest])\n",
    "            else:\n",
    "                neg_final = neg_rand\n",
    "\n",
    "            label_instances.append({\n",
    "                \"sample_idx\": i,\n",
    "                \"label\": label,\n",
    "                \"desc_key\": dx,\n",
    "                \"desc\": desc,\n",
    "                \"pos_spans\": pos_spans,\n",
    "                \"neg_spans\": neg_final,\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"tokenized\": enc,\n",
    "        \"label_instances\": label_instances\n",
    "    }\n",
    "def propose_spans(offset_mapping, max_span_len=12):\n",
    "    # offset_mapping: list[(cs,ce)] token->char\n",
    "    spans = []\n",
    "    n = len(offset_mapping)\n",
    "    for i in range(n):\n",
    "        if offset_mapping[i] == (0,0):  # padding\n",
    "            continue\n",
    "        for j in range(i, min(n, i+max_span_len)):\n",
    "            cs, _ = offset_mapping[i]\n",
    "            _, ce = offset_mapping[j]\n",
    "            if ce <= cs: \n",
    "                continue\n",
    "            spans.append((cs, ce))\n",
    "    # 可去重\n",
    "    spans = list(dict.fromkeys(spans))\n",
    "    return spans\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def sample_desc(label_desc_map, label):\n",
    "    defs = label_desc_map[label]  # {\"D1\":..., ...}\n",
    "    k = random.choice(list(defs.keys()))  # D1~D6\n",
    "    return k, defs[k]\n",
    "\n",
    "def overlap(a,b):\n",
    "    (s1,e1),(s2,e2)=a,b\n",
    "    return not (e1 <= s2 or e2 <= s1)\n",
    "from collections import defaultdict\n",
    "\n",
    "def group_by_label(entities):\n",
    "    mp = defaultdict(list)\n",
    "    for e in entities:\n",
    "        mp[e[\"label\"]].append((e[\"start\"], e[\"end\"]))\n",
    "    return mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75b924c-5b34-4039-9a32-99876dd117cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 612/612 [00:04<00:00, 152.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "parent_dir = \"../dataset/\"\n",
    "describe_filename_list = []\n",
    "for item in os.listdir(parent_dir):\n",
    "    if item.startswith(\"instruct_uie_ner_converted_description_\"):\n",
    "        describe_filename_list.append(item)\n",
    "data = []\n",
    "for item in tqdm.tqdm( describe_filename_list):\n",
    "    file_path = os.path.join(parent_dir, item)\n",
    "    with open(file_path,\"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "    for line in content:\n",
    "        obj = json.loads(line.strip())\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487b9378-2526-4a28-a9cf-cc9f4c4580ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/data/model_hub/mdeberta-v3-base/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "169ab07a-0f23-41e7-90c2-b4af218eb8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'texts': ['<resources>\\n    <!-- Example customization of dimensions originally defined in res/values/dimens.xml\\n         (such as screen margins) for screens with more than 820dp of available width. This\\n         would include 7\" and 10\" devices in landscape (~960dp and ~1280dp respectively). -->\\n    <dimen name=\"activity_horizontal_margin\">64dp</dimen>\\n</resources>',\n",
       "  'Introduction {#sec1}\\n============\\n\\nIn 2017, \\\\~37 million people were living with HIV worldwide, with 1.8 million new infections.^[@cit0001]^ HIV incidence is declining worldwide, but is unlikely to reach the UNAIDS target of \\\\<500,000 new infections by 2020.^[@cit0002]^ Steep reductions in incidence are needed to curb the HIV/AIDS epidemic.'],\n",
       " 'tokenized': {'input_ids': tensor([[     1,   1043, 128743,    670,    260, 102962,    260,  43141,  24088,\n",
       "           14535,    305,    260,  98979,   4704,    485,    260,  54629,    282,\n",
       "            8602,    276,  22346,    276,    286, 102474,    261,  11395,    275,\n",
       "          103545,    528,  10989,    260,   2017,    264,    272,    333,  10989,\n",
       "             264,    515,   1098,   2422,  91114,    286,    326,    305,   4636,\n",
       "             260,   1494,    261,   1495,    260,   2221,   9453,    618,    312,\n",
       "             306,    476,    312,  33822,    282,    260,  33365, 148760,  16978,\n",
       "             286,    326,    306,   3268,  79861,    286,    326,    260,  96113,\n",
       "             484,  46996,   1043,   4549,    279,   6536,    888, 150182,    291,\n",
       "          106888,    291,   2017,   5721,   3951,    286,    326,   2838,   4549,\n",
       "             279,    670,  12524, 128743,    670,      2,      0,      0,      0,\n",
       "               0,      0],\n",
       "         [     1,  73199,    786,    718,  32679, 101914,   5637, 214245,    564,\n",
       "            7326,    260,    541,   1703,   5820,   8382,   2560,   2110,  15557,\n",
       "             515,  46365,    260,  26177,    262,    515,  22438,   8382,   1547,\n",
       "             260,  63447,    264,    261,   3431,    766,   1062,  28739,  37267,\n",
       "             440,   3431,  46365,    260, 173935,    266,    340,  66917,    348,\n",
       "             260,  26177,    262,   1157,    340,    336,   3455,    485,    289,\n",
       "             260,  11126,    288,  10970,  78514,  13297,    305,    260,    541,\n",
       "            2710, 100020,   1547,    260,  63447,    264,    456,  22502,   3431,\n",
       "             766,   1062,  28739,  59305,    440,   3431,  18788,    266,    326,\n",
       "             260,  48321,    264,    282,    260, 173935,    266,    419,  19196,\n",
       "             289,  14088,    317,    288,  46365,    276,  78514,  76253,    298,\n",
       "             261,      2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[[  0,   0],\n",
       "          [  0,   1],\n",
       "          [  1,  10],\n",
       "          [ 10,  11],\n",
       "          [ 15,  16],\n",
       "          [ 16,  20],\n",
       "          [ 20,  21],\n",
       "          [ 21,  28],\n",
       "          [ 28,  35],\n",
       "          [ 35,  42],\n",
       "          [ 42,  45],\n",
       "          [ 45,  46],\n",
       "          [ 46,  56],\n",
       "          [ 56,  65],\n",
       "          [ 65,  67],\n",
       "          [ 67,  68],\n",
       "          [ 68,  75],\n",
       "          [ 75,  78],\n",
       "          [ 78,  82],\n",
       "          [ 82,  83],\n",
       "          [ 83,  89],\n",
       "          [ 89,  90],\n",
       "          [ 90,  91],\n",
       "          [ 91,  96],\n",
       "          [ 96,  97],\n",
       "          [ 97, 100],\n",
       "          [109, 111],\n",
       "          [111, 115],\n",
       "          [115, 118],\n",
       "          [118, 125],\n",
       "          [125, 126],\n",
       "          [126, 132],\n",
       "          [132, 133],\n",
       "          [133, 134],\n",
       "          [134, 138],\n",
       "          [138, 145],\n",
       "          [145, 146],\n",
       "          [146, 151],\n",
       "          [151, 156],\n",
       "          [156, 161],\n",
       "          [161, 165],\n",
       "          [165, 166],\n",
       "          [166, 167],\n",
       "          [167, 170],\n",
       "          [170, 180],\n",
       "          [180, 181],\n",
       "          [181, 186],\n",
       "          [186, 187],\n",
       "          [187, 192],\n",
       "          [201, 202],\n",
       "          [202, 207],\n",
       "          [207, 215],\n",
       "          [215, 217],\n",
       "          [217, 218],\n",
       "          [218, 222],\n",
       "          [222, 225],\n",
       "          [225, 226],\n",
       "          [226, 234],\n",
       "          [234, 237],\n",
       "          [237, 238],\n",
       "          [238, 247],\n",
       "          [247, 250],\n",
       "          [250, 253],\n",
       "          [253, 254],\n",
       "          [254, 255],\n",
       "          [255, 259],\n",
       "          [259, 261],\n",
       "          [261, 265],\n",
       "          [265, 266],\n",
       "          [266, 267],\n",
       "          [267, 268],\n",
       "          [268, 280],\n",
       "          [280, 282],\n",
       "          [282, 286],\n",
       "          [290, 292],\n",
       "          [292, 295],\n",
       "          [295, 297],\n",
       "          [297, 302],\n",
       "          [302, 304],\n",
       "          [304, 312],\n",
       "          [312, 313],\n",
       "          [313, 323],\n",
       "          [323, 324],\n",
       "          [324, 330],\n",
       "          [330, 332],\n",
       "          [332, 334],\n",
       "          [334, 335],\n",
       "          [335, 336],\n",
       "          [336, 338],\n",
       "          [338, 341],\n",
       "          [341, 343],\n",
       "          [343, 344],\n",
       "          [344, 347],\n",
       "          [347, 356],\n",
       "          [356, 357],\n",
       "          [  0,   0],\n",
       "          [  0,   0],\n",
       "          [  0,   0],\n",
       "          [  0,   0],\n",
       "          [  0,   0],\n",
       "          [  0,   0]],\n",
       " \n",
       "         [[  0,   0],\n",
       "          [  0,  12],\n",
       "          [ 12,  14],\n",
       "          [ 14,  15],\n",
       "          [ 15,  18],\n",
       "          [ 18,  20],\n",
       "          [ 20,  23],\n",
       "          [ 23,  33],\n",
       "          [ 34,  37],\n",
       "          [ 37,  43],\n",
       "          [ 43,  44],\n",
       "          [ 44,  45],\n",
       "          [ 45,  46],\n",
       "          [ 46,  48],\n",
       "          [ 48,  56],\n",
       "          [ 56,  63],\n",
       "          [ 63,  68],\n",
       "          [ 68,  75],\n",
       "          [ 75,  80],\n",
       "          [ 80,  84],\n",
       "          [ 84,  85],\n",
       "          [ 85,  94],\n",
       "          [ 94,  95],\n",
       "          [ 95, 100],\n",
       "          [100, 104],\n",
       "          [104, 112],\n",
       "          [112, 116],\n",
       "          [116, 117],\n",
       "          [117, 126],\n",
       "          [126, 127],\n",
       "          [127, 128],\n",
       "          [128, 129],\n",
       "          [129, 130],\n",
       "          [130, 131],\n",
       "          [131, 134],\n",
       "          [134, 138],\n",
       "          [138, 139],\n",
       "          [139, 140],\n",
       "          [140, 144],\n",
       "          [144, 145],\n",
       "          [145, 153],\n",
       "          [153, 154],\n",
       "          [154, 157],\n",
       "          [157, 164],\n",
       "          [164, 167],\n",
       "          [167, 168],\n",
       "          [168, 177],\n",
       "          [177, 178],\n",
       "          [178, 182],\n",
       "          [182, 185],\n",
       "          [185, 188],\n",
       "          [188, 192],\n",
       "          [192, 194],\n",
       "          [194, 197],\n",
       "          [197, 198],\n",
       "          [198, 203],\n",
       "          [203, 207],\n",
       "          [207, 210],\n",
       "          [210, 214],\n",
       "          [214, 221],\n",
       "          [221, 224],\n",
       "          [224, 225],\n",
       "          [225, 226],\n",
       "          [226, 227],\n",
       "          [227, 234],\n",
       "          [234, 238],\n",
       "          [238, 239],\n",
       "          [239, 248],\n",
       "          [248, 249],\n",
       "          [249, 252],\n",
       "          [252, 258],\n",
       "          [258, 259],\n",
       "          [259, 260],\n",
       "          [260, 261],\n",
       "          [261, 264],\n",
       "          [264, 268],\n",
       "          [268, 269],\n",
       "          [269, 270],\n",
       "          [270, 274],\n",
       "          [274, 275],\n",
       "          [275, 276],\n",
       "          [276, 277],\n",
       "          [277, 286],\n",
       "          [286, 287],\n",
       "          [287, 290],\n",
       "          [290, 291],\n",
       "          [291, 299],\n",
       "          [299, 300],\n",
       "          [300, 304],\n",
       "          [304, 311],\n",
       "          [311, 314],\n",
       "          [314, 318],\n",
       "          [318, 319],\n",
       "          [319, 323],\n",
       "          [323, 327],\n",
       "          [327, 328],\n",
       "          [328, 332],\n",
       "          [332, 340],\n",
       "          [340, 341],\n",
       "          [341, 342],\n",
       "          [  0,   0]]])},\n",
       " 'label_instances': [{'sample_idx': 0,\n",
       "   'label': 'dimension value',\n",
       "   'desc_key': 'D2',\n",
       "   'desc': \"The text inside a dimen XML element that specifies a measurement, consisting of a number and a dimensional unit such as 'dp', and located between the opening and closing tags.\",\n",
       "   'pos_spans': [(332, 336)],\n",
       "   'neg_spans': [(tensor(226), tensor(253))]},\n",
       "  {'sample_idx': 0,\n",
       "   'label': 'dimension name',\n",
       "   'desc_key': 'D5',\n",
       "   'desc': \"Quoted identifier following 'name=' in a dimen opening tag.\",\n",
       "   'pos_spans': [(304, 330)],\n",
       "   'neg_spans': [(tensor(187), tensor(222))]},\n",
       "  {'sample_idx': 0,\n",
       "   'label': 'file path',\n",
       "   'desc_key': 'D5',\n",
       "   'desc': \"Res-relative path inside a comment, prefixed by 'res/' and embedded in explanatory text.\",\n",
       "   'pos_spans': [(79, 100)],\n",
       "   'neg_spans': [(tensor(151), tensor(180))]},\n",
       "  {'sample_idx': 1,\n",
       "   'label': 'disease',\n",
       "   'desc_key': 'D4',\n",
       "   'desc': \"Include only the standalone three-letter initialism when it directly modifies terms like 'incidence', 'epidemic', or 'infections'; exclude any surrounding modifiers, parentheses, or citation markers.\",\n",
       "   'pos_spans': [(81, 84), (141, 144), (324, 327)],\n",
       "   'neg_spans': [(tensor(269), tensor(304)),\n",
       "    (tensor(84), tensor(129)),\n",
       "    (tensor(203), tensor(227))]},\n",
       "  {'sample_idx': 1,\n",
       "   'label': 'organization',\n",
       "   'desc_key': 'D2',\n",
       "   'desc': 'A formal entity name rendered as a capitalized multi-letter acronym, occurring alongside phrases specifying numerical targets and year-bound objectives.',\n",
       "   'pos_spans': [(208, 214)],\n",
       "   'neg_spans': [(tensor(75), tensor(117))]}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_batch(data[0:2],tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb21e50c-74e2-45b2-a8ff-01dc8f20a568",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85a9be-11db-4e38-aa54-bb276c3564bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gliner",
   "language": "python",
   "name": "gliner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
